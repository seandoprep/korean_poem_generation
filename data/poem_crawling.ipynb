{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c6a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "poem_crawled_data.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73c900",
   "metadata": {},
   "source": [
    "디카시 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7172e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링할 url 생성하는 함수\n",
    "def makeUrl(webname, start_num, end_num):\n",
    "    url_list = []\n",
    "    \n",
    "    if webname == 'dica': # 게시판 : 창작 디카시 1, 총 시 개수 : 21800\n",
    "        for i in range(start_num,end_num+1): \n",
    "            url_list.append('https://m.cafe.daum.net/dicapoetry/1aSh/' + str(i))\n",
    "    \n",
    "    elif webname == 'poemlove': # 시사랑, 총 시 개수 : 200000개 이상(일단 50000까지 수집 예정)\n",
    "        for i in range(start_num,end_num+1): \n",
    "            url_list.append('http://www.poemlove.co.kr/bbs/board.php?bo_table=tb01&wr_id=' + str(i))\n",
    "        \n",
    "    return url_list\n",
    "\n",
    "# 크롤링 코드\n",
    "def crawlPage(webname, start_num, end_num):\n",
    "    urls = makeUrl(webname, start_num, end_num)\n",
    "    titles, contents, imgs = [], [], []\n",
    "    \n",
    "    for url in urls:\n",
    "        webpage = requests.get(url)\n",
    "        \n",
    "        if webpage.status_code == 200:\n",
    "            html = webpage.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            if webname == 'dica':\n",
    "            \n",
    "                # 제목\n",
    "                title = soup.title.text\n",
    "                dash_index = title.find('-')\n",
    "                title = title[:dash_index-1]\n",
    "                titles.append(title)\n",
    "\n",
    "                # 본문\n",
    "                content = soup.article\n",
    "                content = content.find(attrs = {'class' : 'tx-content-container'}).text\n",
    "                contents.append(content)\n",
    "\n",
    "                # 이미지\n",
    "                img_url = soup.img['src']\n",
    "                imgs.append(img_url)\n",
    "            \n",
    "            elif webname == 'poemlove':\n",
    "\n",
    "                # 제목\n",
    "                title = soup.title.text\n",
    "                if title:\n",
    "                    titles.append(title)\n",
    "                else:\n",
    "                    titles.append('null')\n",
    "                \n",
    "                # 본문\n",
    "                content = soup.article\n",
    "                if content:\n",
    "                    content = content.find(attrs = {'class' : 'view-content'}).text\n",
    "                    contents.append(content)\n",
    "                else:\n",
    "                    contents.append('null')\n",
    "                \n",
    "        else: \n",
    "            pass\n",
    "    \n",
    "    if webname == 'dica':\n",
    "        poem_df = pd.DataFrame({'title' : titles,\n",
    "                                'content' : contents,\n",
    "                                'img_url' : imgs})    \n",
    "        \n",
    "    elif webname == 'poemlove':\n",
    "        poem_df = pd.DataFrame({'title' : titles,\n",
    "                                'content' : contents})    \n",
    "    \n",
    "    return poem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb6342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crawlPage('dica', 1,21800), _\n",
    "df.to_csv('C:\\\\Users\\\\82103\\\\Desktop\\\\연세대학교\\\\YAI\\\\dica_poem_crawled_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837c038",
   "metadata": {},
   "source": [
    "시사랑 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32883cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = crawlPage('poemlove', 40001, 50000)\n",
    "df.to_csv('C:\\\\Users\\\\82103\\\\Desktop\\\\연세대학교\\\\YAI\\\\poemlove_poem_crawled_data40001_50000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "931718ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링할 url 생성하는 함수\n",
    "def makeUrl(start_num, end_num):\n",
    "    url_list = []\n",
    "    \n",
    "    for i in range(start_num,end_num+1): \n",
    "        url_list.append('https://m.cafe.daum.net/dicapoetry/1aSh/' + str(i))\n",
    "\n",
    "    return url_list\n",
    "\n",
    "# 크롤링 코드\n",
    "def crawlPage(start_num, end_num):\n",
    "    urls = makeUrl(start_num, end_num)\n",
    "    imgs = []\n",
    "    \n",
    "    for url in urls:\n",
    "        webpage = requests.get(url)\n",
    "        \n",
    "        if webpage.status_code == 200:\n",
    "            html = webpage.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            # 이미지\n",
    "            img_url = soup.find_all('img')[1]['src']\n",
    "            imgs.append(img_url)\n",
    "                \n",
    "        else: \n",
    "            pass\n",
    "    \n",
    "\n",
    "    poem_df = pd.DataFrame({'img_url' : imgs})    \n",
    "    \n",
    "    return poem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd547b7e",
   "metadata": {},
   "source": [
    "dica 시 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a04d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82103\\AppData\\Local\\Temp/ipykernel_17912/3157832233.py:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\"C:\\\\Users\\\\82103\\\\Desktop\\\\Deepdaiv\\\\chromedriver.exe\", chrome_options=options)\n",
      "C:\\Users\\82103\\AppData\\Local\\Temp/ipykernel_17912/3157832233.py:10: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(\"C:\\\\Users\\\\82103\\\\Desktop\\\\Deepdaiv\\\\chromedriver.exe\", chrome_options=options)\n",
      "C:\\Users\\82103\\AppData\\Local\\Temp/ipykernel_17912/3157832233.py:51: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  images = driver.find_elements_by_tag_name('img')\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 크롬 브라우저를 실행합니다.\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--disable-extensions')\n",
    "driver = webdriver.Chrome(\"C:\\\\Users\\\\82103\\\\Desktop\\\\Deepdaiv\\\\chromedriver.exe\", chrome_options=options)\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "# 이미지 주소를 저장할 리스트를 생성합니다.\n",
    "urls = makeUrl(14115,21917)\n",
    "titles = []\n",
    "contents = []\n",
    "img_urls = []\n",
    "\n",
    "# 이미지를 가져올 웹 페이지를 엽니다.\n",
    "for url in urls:\n",
    "    \n",
    "    # 시간 텀 두기\n",
    "    seed = np.random.randint(100)\n",
    "    np.random.seed(seed)\n",
    "    time_sleep = np.random.randint(3)\n",
    "    time.sleep(time_sleep)\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    webpage = requests.get(url)\n",
    "    html = webpage.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    try: \n",
    "        title = soup.title.text\n",
    "        dash_index = title.find('-')\n",
    "        title = title[:dash_index-1]\n",
    "        titles.append(title)\n",
    "    except:\n",
    "        titles.append('null')\n",
    "        \n",
    "    try:\n",
    "        content = soup.article\n",
    "        content = content.find(attrs = {'class' : 'tx-content-container'}).text\n",
    "        contents.append(content)\n",
    "    except:\n",
    "        contents.append('null')\n",
    "    \n",
    "    # 모든 이미지 태그를 가져옵니다.    \n",
    "    try:\n",
    "        images = driver.find_elements_by_tag_name('img')\n",
    "\n",
    "        # 각 이미지 태그에서 src 속성을 가져와서 리스트에 추가합니다.\n",
    "        img_urls.append(images[1].get_attribute('src'))\n",
    "    except:\n",
    "        img_urls.append('null')\n",
    "        \n",
    "df = pd.DataFrame({'title' : titles, 'content' : contents, 'img_url' : img_urls})\n",
    "df.to_csv('C:\\\\Users\\\\82103\\\\Desktop\\\\연세대학교\\\\YAI\\\\dica_poem_crawled_14115_21917.csv')\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1af0aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'title' : titles, 'content' : contents, 'img_url' : img_urls})\n",
    "df.to_csv('C:\\\\Users\\\\82103\\\\Desktop\\\\연세대학교\\\\YAI\\\\dica_poem_crawled_1_14114.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
